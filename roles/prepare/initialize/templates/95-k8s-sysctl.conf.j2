# 修复ipvs模式下长连接timeout问题 小于900即可
# tcp_keepalive_time需要低于ipvs中的tcp_timeout时长,一般情况下要低于kube-proxy lvs的900s
net.ipv4.tcp_keepalive_time = 600  #此参数表示TCP发送keepalive探测消息的间隔时间(秒)
net.ipv4.tcp_keepalive_intvl = 30  #tcp检查间隔时间(keepalive探测包的发送间隔)
net.ipv4.tcp_keepalive_probes = 10 #tcp检查次数(如果对方不予应答,探测包的发送次数)
# socket buffer优化
net.ipv4.tcp_wmem = 4096 524287 16777216
net.ipv4.tcp_rmem = 4096 524287 16777216
net.ipv4.tcp_mem = 381462 508616 762924
net.core.rmem_default = 8388608
net.core.rmem_max = 26214400 # 读写 buffer 调到 25M 避免大流量时导致 buffer 满而丢包 "netstat -s" 可以看到 receive buffer errors 或 send buffer errors
net.core.wmem_max = 26214400 # 最大的TCP数据发送窗口大小
# 可能会出现两种异常情况：
# 1.对端服务器发完最后一个 Fin 包,没有收到当前服务器返回最后一个 Ack,又重发了 Fin 包,因为新的 TimeWait 没有办法创建 ,这个连接在当前服务器上就消失了,对端服务器将会收到一个 Reset 包。因为这个连接是明确要关闭的,所以收到一个 Reset 也不会有什么大问题。(但是违反了 TCP/IP 协议)
# 2.因为这个连接在当前服务器上消失,那么刚刚释放的端口可能被立刻使用,如果这时对端服务器没有释放连接,当前服务器就会收到对端服务器发来的 Reset 包。如果当前服务器是代理服务器,就可能会给用户返回 502 错误。(这种异常对服务或者用户是有影响的)
net.ipv4.tcp_max_tw_buckets = 360000 #配置服务器 TIME_WAIT 数量,这个优化意义不大
net.ipv4.tcp_timestamps = 1  # 通常默认本身是开启的
net.ipv4.tcp_syncookies = 1  # 此参数应该设置为1,防止SYN Flood
net.ipv4.tcp_tw_reuse = 1 # 仅对客户端有效果,对于高并发客户端,可以复用TIME_WAIT连接端口,避免源端口耗尽建连失败
net.ipv4.ip_local_port_range = 1024 65535 # 对于高并发客户端,加大源端口范围,避免源端口耗尽建连失败(确保容器内不会监听源端口范围的端口)
net.ipv4.tcp_fin_timeout = 30 # 缩短TIME_WAIT时间,加速端口回收
# 没有启用syncookies的情况下,syn queue(半连接队列)大小除了受somaxconn限制外,也受这个参数的限制,默认1024,优化到65535,避免在高并发场景下丢包
net.ipv4.tcp_max_syn_backlog = 65535
# 服务器SYN+ACK报文重试次数,尽快释放等待资源
net.ipv4.tcp_synack_retries = 2 #表示回应第二个握手包(SYN+ACK包)给客户端IP后,如果收不到第三次握手包(ACK包),进行重试的次数(默认为5)
net.ipv4.neigh.default.gc_stale_time = 120 #ARP缓存条目超时
# 以下三个参数是 arp 缓存的 gc 阀值,相比默认值提高了,避免在某些场景下arp缓存溢出导致网络超时,参考:https://k8s.imroc.io/troubleshooting/cases/arp-cache-overflow-causes-healthcheck-failed
# 配置arp cache 大小
net.ipv4.neigh.default.gc_thresh1 = 8192
net.ipv6.neigh.default.gc_thresh1=8192
# 存在于ARP高速缓存中的最少层数,如果少于这个数,垃圾收集器将不会运行。缺省值是128。
# 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前,允许记录数超过这个数字 5 秒。缺省值是 512。
net.ipv4.neigh.default.gc_thresh2 = 32768
net.ipv6.neigh.default.gc_thresh2 = 32768
# 保存在 ARP 高速缓存中的最多记录的硬限制,一旦高速缓存中的数目高于此,垃圾收集器将马上运行。缺省值是1024。
net.ipv4.neigh.default.gc_thresh3 = 65536
net.ipv6.neigh.default.gc_thresh3 = 65536
net.ipv4.neigh.default.gc_interval = 120
net.ipv6.neigh.default.gc_interval = 120
net.ipv4.ip_forward = 1 # 其值为0,说明禁止进行IP转发;如果是1,则说明IP转发功能已经打开。
net.ipv4.conf.default.rp_filter = 0 #不开启源地址校验
net.ipv4.conf.default.accept_source_route = 0
net.ipv4.conf.all.promote_secondaries = 1
net.ipv4.conf.default.promote_secondaries = 1
# 预留端口避免被占用
net.ipv4.ip_local_reserved_ports = 30000-32767
net.ipv4.conf.all.rp_filter = 0 #默认为1,系统会严格校验数据包的反向路径,可能导致丢包
net.ipv4.conf.all.accept_source_route = 0
net.ipv4.conf.default.arp_announce = 2 #始终使用与目的IP地址对应的最佳本地IP地址作为ARP请求的源IP地址
net.ipv4.conf.lo.arp_announce = 2      #始终使用与目的IP地址对应的最佳本地IP地址作为ARP请求的源IP地址
net.ipv4.conf.all.arp_announce = 2     #始终使用与目的IP地址对应的最佳本地IP地址作为ARP请求的源IP地址
# 记录的那些尚未收到客户端确认信息的连接请求的最大值。对于有128M内存的系统而言,缺省值是1024,小内存的系统则是128
net.ipv4.tcp_max_orphans = 32768
{% if set_fact_kernel_version is version('4.12', '<') %}
net.ipv4.tcp_tw_recycle = 0
{% endif %}
# 不禁用 ipv6
net.ipv6.conf.all.disable_ipv6 = 0     #禁用IPv6,修为0为启用IPv6
net.ipv6.conf.default.disable_ipv6 = 0 #禁用IPv6,修为0为启用IPv6
net.ipv6.conf.lo.disable_ipv6 = 0      #禁用IPv6,修为0为启用IPv6
# 哈希表大小(只读)(64位系统、8G内存默认 65536,16G翻倍,如此类推)
# 每个网络接口接收数据包的速率比内核处理这些包的速率快时,允许送到队列的数据包的最大数目
net.core.netdev_max_backlog = 262144  # 每CPU网络设备积压队列长度
# conntrack优化
net.netfilter.nf_conntrack_tcp_be_liberal = 1 # 容器环境下,开启这个参数可以避免 NAT 过的 TCP 连接 带宽上不去。
net.netfilter.nf_conntrack_tcp_loose = 1
# 允许的最大跟踪连接条目,是在内核内存中netfilter可以同时处理的"任务"(连接跟踪条目)
net.netfilter.nf_conntrack_max = 10485760 #连接跟踪表的大小,建议根据内存计算该值CONNTRACK_MAX = RAMSIZE (in bytes) / 16384 / (x / 32),并满足nf_conntrack_max=4*nf_conntrack_buckets,默认262144
# 每个网络接口接收数据包的速率比内核处理这些包的速率快时,允许送到队列的数据包的最大数目。维持通过NAT维持TCP长连接的优化,注意kube-proxy会修改此参数
net.netfilter.nf_conntrack_tcp_timeout_established = 3600
net.netfilter.nf_conntrack_buckets = 655360
net.netfilter.nf_conntrack_tcp_timeout_fin_wait = 30
net.netfilter.nf_conntrack_tcp_timeout_time_wait = 300
net.netfilter.nf_conntrack_tcp_timeout_close_wait = 1
# 要求iptables不对bridge的数据进行处理
net.bridge.bridge-nf-call-arptables = 1 #是否在arptables的FORWARD中过滤网桥的ARP包
net.bridge.bridge-nf-call-ip6tables = 1 # 是否在ip6tables链中过滤IPv6包
net.bridge.bridge-nf-call-iptables = 1  # 二层的网桥在转发包时也会被iptables的FORWARD规则所过滤,这样有时会出现L3层的iptables rules去过滤L2的帧的问题
net.ipv4.tcp_dsack = 1
net.ipv4.tcp_sack = 0
net.ipv4.tcp_fack = 1
net.ipv4.conf.all.forwarding = 1
net.ipv4.conf.default.forwarding = 1
net.ipv4.conf.all.send_redirects = 0
net.ipv4.conf.default.send_redirects = 0
net.ipv4.conf.all.accept_redirects = 0
net.ipv4.conf.default.accept_redirects = 0
net.ipv4.conf.all.secure_redirects = 0
net.ipv4.conf.default.secure_redirects = 0
net.ipv4.conf.all.bootp_relay = 0
net.ipv4.conf.all.proxy_arp = 0
net.ipv4.icmp_echo_ignore_broadcasts = 1
net.ipv4.icmp_ignore_bogus_error_responses = 1
net.ipv4.tcp_rfc1337 = 1
net.ipv4.tcp_congestion_control = cubic
net.core.default_qdisc = pfifo_fast
net.ipv4.tcp_ecn = 2
net.ipv4.tcp_reordering = 3
net.ipv4.tcp_retries2 = 8
net.ipv4.tcp_retries1 = 3
net.ipv4.tcp_no_metrics_save = 1
net.ipv4.tcp_slow_start_after_idle = 0
# 表示socket监听(listen)的backlog上限,也就是就是socket的监听队列(accept queue),当一个tcp连接尚未被处理或建立时(半连接状态)
#会保存在这个监听队列,默认为 128,在高并发场景下偏小,优化到 16384。参考 https://imroc.io/posts/kubernetes-overflow-and-drop/
net.core.somaxconn = 16384
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_adv_win_scale = 2
net.core.wmem_default = 524287
net.core.optmem_max = 524287
net.ipv4.tcp_fastopen = 3
net.ipv4.route.flush = 1
#tcp_keepalive_time+ tcp_keepalive_interval * tcp_keepalive_max_retry + 2msl取整
#https://www.xinmeow.com/2020/05/04/iptables-nf_conntrack-%E6%9D%A1%E7%9B%AE%E7%9A%84%E8%80%81%E5%8C%96%E6%97%B6%E9%97%B4%E5%9B%A0%E8%AF%A5%E8%BF%9E%E6%8E%A5%E5%8F%91%E7%94%9F%E4%B8%A2%E5%8C%85%E8%80%8C%E5%8F%98%E7%9F%AD%EF%BC%8C/
net.netfilter.nf_conntrack_tcp_timeout_max_retrans = 720

vm.max_map_count = 655360
vm.swappiness = 0 # 禁止使用 swap 空间,只有当系统 OOM 时才允许使用它
vm.overcommit_memory = 1 # 不检查物理内存是否够用
vm.panic_on_oom = 0 # 开启 OOM
vm.oom_dump_tasks = 1
# 磁盘 IO 优化: https://www.cnblogs.com/276815076/p/5687814.html
vm.dirty_background_ratio = 5
vm.min_free_kbytes = {{ (ansible_memtotal_mb*1024/100*5)|int }}
vm.dirty_expire_centisecs = 50
vm.dirty_ratio = 60
vm.dirty_writeback_centisecs = 50
vm.dirtytime_expire_seconds = 43200

# max-file 表示系统级别的能够打开的文件句柄的数量, 一般如果遇到文件句柄达到上限时,会碰到"Too many open files"或者Socket/File: Can't open so many files等错误。
fs.file-max = 52706963 # 提升文件句柄上限,像 nginx 这种代理,每个连接实际分别会对 downstream 和 upstream 占用一个句柄,连接量大的情况下句柄消耗就大。
fs.nr_open = 52706963
# 每个网络接口接收数据包的速率比内核处理这些包的速率快时,允许送到队列的数据包的最大数目。
fs.inotify.max_queued_events = 524288
# 默认值: 128 指定了每一个real user ID可创建的inotify instatnces的数量上限
fs.inotify.max_user_instances = 524288 # 表示同一用户同时最大可以拥有的 inotify 实例 (每个实例可以有很多 watch)
# 默认值: 8192 指定了每个inotify instance相关联的watches的上限
fs.inotify.max_user_watches = 524288 # 表示同一用户同时可以添加的watch数目(watch一般是针对目录,决定了同时同一用户可以监控的目录数量) 默认值 8192 在容器场景下偏小,在某些情况下可能会导致 inotify watch 数量耗尽,使得创建 Pod 不成功或者 kubelet 无法启动成功,将其优化到 524288
fs.protected_hardlinks = 1
fs.protected_symlinks = 1

# kubelet 参数
# kubelet --protect-kernel-defaults = true
kernel.panic = 10
kernel.panic_on_oops = 10
kernel.pid_max = 4194303 #最大进程数
kernel.threads-max = 196605
kernel.core_uses_pid = 1
kernel.msgmnb = 65536
kernel.msgmax = 65536
kernel.softlockup_panic = 1
kernel.sysrq = 1
kernel.numa_balancing = 0
kernel.printk = 5
kernel.ctrl-alt-del = 1
kernel.kptr_restrict = 1
#打开coredns
kernel.core_pattern = /var/log/core.%e.%p.%t
kernel.shmmax = {{ (ansible_memtotal_mb*1024*1024/2)|int }}
kernel.shmall = {{ (ansible_memtotal_mb*1024*1024/2/4096)|int }}
kernel.msgmnb = 65536
kernel.msgmax = 65536
